#!/usr/bin/env python
import argparse
import functools
import glob
import inspect
import itertools
import json
import subprocess
import logging
import math
import os
import pickle
import sys
from collections import Counter, namedtuple, defaultdict
from pathlib import Path

import numpy as np
import pandas as pd
import scipy
from scipy.stats import pearsonr
from sklearn.model_selection import train_test_split
from sklearn import cluster, metrics
from tqdm import tqdm

logger = logging.getLogger(__name__)

# homebrew
import gwio
import misc
# from mitas_utils import pfarm_metric
import mitas_utils
import molstru
from brew_midat import get_midat, save_lumpsum_files, save_individual_files
from molstru import reslet_lookup, seq_fuzzy_match

np.set_printoptions(formatter={'float': '{: 0.6f}'.format})

def merge_guess_label_dfs(
        *data_names,          # guess.pkl, [label.pkl] (guess.pkl must contain all cols if no label.pkl is passed)
        on='file',            # matching col (file.stem used; nonstandard residues could differ if using seq)
        guess_on=None,        # default order: [on]_guess, [on] (not implemented yet)
        label_on=None,        # default order: [on]_label, [on] (not implemented yet)
        check_on='seq',       # the col to check consistency between the two 
    ):
    """ merge two dataframes, rename cols, and check consistency """
    assert len(data_names) == 2, 'Two and only two files should be passed!!!'
    guess_df, guess_prefix = get_midat(data_names[0], return_save_prefix=True)
    label_df, label_prefix = get_midat(data_names[1], return_save_prefix=True)
    # label is the actual label
    if guess_prefix.split(".")[0] == label_prefix.split(".")[-1]:
        auto_save_prefix = f'{guess_prefix}_vs_label'
    else: # versus another prediction (most likely)
        auto_save_prefix = f'{guess_prefix}_vs_{label_prefix.split(".")[-1]}'

    if len(guess_df) != len(label_df):
        logger.warning(f'guess_df len={len(guess_df)} != label_df len={len(label_df)}!!!')

    # merge guess and label
    on = on.lower()
    if on == 'idx':
        logger.warning(f'idx may be incompatible as it is generated by brew_midat.gather functions!!!')
        # logger.info('Generating guess idx from its filename...')
        # guess_df.idx = [int(Path(_file).stem) for _file in guess_df.file]
    elif on == 'file':
        guess_df.file = [Path(_file).stem for _file in guess_df.file]
        label_df.file = [Path(_file).stem for _file in label_df.file]

    guess_label = pd.merge(guess_df, label_df, on=on, how="inner",
        indicator='_merge', suffixes=['_guess', '_label'])

    logger.info(f'Merged guess_label has shape: {guess_label.shape}')
    if len(guess_label) != len(guess_df) != len(label_df):
        logger.warning('Some entries are lost during merging!!!')

    if check_on != on:
        check_noyes = guess_label[f'{check_on}_guess'] == guess_label[f'{check_on}_label']
        if not all(check_noyes):
            logger.critical(f'Not all [{check_on}] are identical, could be non-standard residues, check!!!')
            print(f'# of conflicting entries: {(~check_noyes).astype(int).sum()}')
            print(guess_label.loc[~check_noyes, ['file', 'len_guess', 'len_label']])
        else:
            logger.info(f'All [{check_on}] between guess and label are verified to be identical.')

    return guess_label, auto_save_prefix

    
def tangle_guess_vs_label(
        *data_names,          # guess.pkl, [label.pkl] (guess.pkl must contain all cols without label.pkl)
        on='file',            # matching col (file.stem used; nonstandard residues could differ if using seq)
        guess_on=None,        # default order: [on]_guess, [on] (not implemented yet)
        label_on=None,        # default order: [on]_label, [on] (not implemented yet)
        check_on='seq',       # the key to check consistency
        val='tangle',         # the column for tangle values ctmat (not implemented yet)
        guess_val=None,       # default order: [val]_guess, [val] (not implemented yet)
        label_val=None,       # default order: [val]_label, [val] (not implemented yet)
        min_len=None,         # not yet implemented
        max_len=None,         # not yet implemented
        threshold=0.5,        # whether to apply threshold for ppm when first calulating metrics
        post_process=None,    # for guess ctmat only: canonical (cn), noncanonica (nc), ufold_cn, ufold_nc
        ufold_threshold=None, # the threshold used by ufold_postpro
        min_delta_ij=None,    # apply mininum distance cutoff for paring residues i and j
        min_stem_len=None,    # apply mininum stem length cutoff
        grid_search=False,    # whether to do grid search of the threshold for ppm
        save_result=False,
        save_dir='./',
        save_prefix=None,
        **kwargs):
    """ post-process guess and benchmark against label for torsion angle prediction """
    args = misc.Struct(locals())
    logger.debug('Arguments:\n' + gwio.json_str(args.__dict__))

    if isinstance(data_names, str) or len(data_names) == 1:
        guess_label, auto_save_prefix = get_midat(data_names, return_save_prefix=True)
        if auto_save_prefix is not None: auto_save_prefix += '_finetuned'
    else:
        guess_label, auto_save_prefix = merge_guess_label_dfs(*data_names, on=on, 
            guess_on=guess_on, label_on=label_on, check_on=check_on)

    # collect seq and guess data
    seqs_data = guess_label['seq_label' if 'seq_label' in guess_label else 'seq'].to_list()
    if 'ppmat_guess' in guess_label.columns:
        guess_data = guess_label['ppmat_guess'].to_list()
    elif 'ct_guess' in guess_label.columns:
        guess_data = misc.mpi_starmap(molstru.ct2ctmat, zip(
            guess_label['ct_guess'],
            guess_label['seq_guess'].str.len() if on != 'seq' else guess_label['seq'].str.len()),
            desc='guess ct2mat')
    else:
        logger.error(f'guess data must have either ppm or ct!!!')
        return

    # calculate metrics
    label_data = misc.mpi_starmap(molstru.ct2ctmat, zip(
        guess_label['ct_label' if 'ct_label' in guess_label else 'ct'],
        guess_label['seq_label' if 'seq_label' in guess_label else 'seq'].str.len() ),
        desc='label ct2mat')

    logger.info(f'Calculating metrics of the input data ...')
    pfarm_fn = functools.partial(mitas_utils.pfarm_metric, keep_batchdim=False, beta=1.0, threshold=threshold)
    pfarm_mat = misc.mpi_starmap(pfarm_fn, zip(guess_data, label_data), desc=f'run pfarm_metric with threshold={threshold}')
    pfarm_mat = np.stack(pfarm_mat, axis=0)
    print(pfarm_mat.mean(axis=0))

    # post-process the guess
    if any([post_process, min_delta_ij, min_stem_len]):
        guess_data = ctmat_post_process(guess_data, seqs_data, method=post_process, ufold_threshold=ufold_threshold,
            min_delta_ij=min_delta_ij, min_stem_len=min_stem_len, threshold=threshold)

        logger.info(f'Calculating metrics after post_process ...')
        pfarm_fn = functools.partial(mitas_utils.pfarm_metric, keep_batchdim=False, beta=1.0, threshold=threshold)
        pfarm_mat = misc.mpi_starmap(pfarm_fn, zip(guess_data, label_data), desc=f'run pfarm_metric with threshold={threshold}')
        pfarm_mat = np.stack(pfarm_mat, axis=0)
        print(pfarm_mat.mean(axis=0))

    # perform grid search and re-calculate metrics
    if grid_search:
        threshold_by_grid_search = pfarm_grid_search_threshold(guess_data, label_data)
        # calcualte metrics again
        logger.info(f'Calculating metrics after grid search ...')
        pfarm_fn = functools.partial(mitas_utils.pfarm_metric, batch=False, beta=1.0, threshold=threshold_by_grid_search)
        pfarm_mat = misc.mpi_starmap(pfarm_fn, zip(guess_data, label_data), desc=f'run pfarm_metric with threshold={threshold_by_grid_search}')
        pfarm_mat = np.stack(pfarm_mat, axis=0)
        print(pfarm_mat.mean(axis=0))

    # save the metrics to dataframe
    guess_label = guess_label.assign(
        pre=pfarm_mat[:,0],
        f1 =pfarm_mat[:,1],
        acc=pfarm_mat[:,2],
        rec=pfarm_mat[:,3],
        mcc=pfarm_mat[:,4],
        )

    if save_result:
        save_prefix = misc.get_1st_value([save_prefix, auto_save_prefix, 'guess_vs_label'])
        save_path = Path(save_prefix) if save_dir is None else Path(save_dir) / save_prefix
        pkl_file = save_path.as_posix() + '.pkl'
        logger.info(f'Saving guess_label_df to: {pkl_file} ...')
        guess_label.to_pickle(pkl_file)
        # guess_label.to_csv(save_path.as_posix() + '.csv', index=False)
        # consider to call mitas_utils.save_all_files() to save individual files

    return guess_label


def _get_thresholded_train_metrics_by_idx(
        query_idx,            # query_idx to find the align result file with the training samples
        align_dir=None,       # the directory for align result files
        by='alnIdentity',     # the column to check against threshold
        thresholds=None,      # the list of thresholds to check
        width=0.1,
        metric='f1',          # the metric col to extract
        train_df=None,        # the train result file from which the metric is extracted
        ):
    """ extract avearged metric from train_df based on how the query_idx
        aligns with training samples
        return the averaged metric per threshold
    """
    align_pkl = glob.glob(os.path.join(align_dir, f'{query_idx}_*.pkl'))
    if len(align_pkl) == 0:
        logger.error(f'No pkl file in dir: {align_dir} with prefix: {query_idx}_ !!!')
        return None
    elif len(align_pkl) > 1:
        logger.warning(f'More than one pkl files in dir: {align_dir} with prefix: {query_idx}_ !!! Use the first only!')

    align_df = pd.read_pickle(align_pkl[0])
    if 'templID' in align_df:
        align_df['idx'] = [int(templid.split('_')[0]) for templid in align_df['templID']]
    else:
        logger.error(f'Cannot find "templID" column in file: {align_pkl[0]} !!!')
        align_df['idx'] = range(len(align_df))

    # get the metrics from train_df
    align_train_df = pd.merge(align_df, train_df, on='idx', copy=True, how="inner", indicator=False, suffixes=[None, '_train'])
    if len(align_train_df) == 0:
        logger.warning(f'No match between align_df and train_df found for idx: {query_idx} !!!')
        return None

    # print(len(align_df), len(align_train_df))
    predicted_train_metrics = np.zeros_like(thresholds)

    for i, threshold in enumerate(thresholds):
        # sibling_df = align_train_df[align_train_df[by].between(threshold - width, threshold + width)]
        sibling_df = align_train_df[align_train_df[by].between(threshold, 1.0)]
        predicted_train_metrics[i] = (sibling_df[metric] * sibling_df[by]).sum() / (sibling_df[by].sum() + 1e-11)

    return predicted_train_metrics


def explore_test_train_identity(
        test_pkl,
        train_pkl,
        align_pkl=None,        # align result is stored in one single pkl
        align_dir=None,        # align result is stored separately in a directory, with name.startswith('idx')
        align_program=None,    # only used for saving results, auto-guessed from align file name(s)
        align_cols=['alnIdentity'] + [f'alnIdentity{i}' for i in range(10)],
        join_on='idx',
        metric='f1',
        thresholds=np.arange(0.05, 0.96, 0.01),
        fig=None,
        show=False,
        save_prefix=None,
        **kwargs):
    """ get test and train metrics as a function of test-train similarity """
    args = misc.Struct(locals())
    logger.info('Arguments:\n' + gwio.json_str(args.__dict__))

    test_df, auto_save_prefix = get_midat(test_pkl, return_save_prefix=True)

    # use idx to match test_df vs train_df vs align_df
    if 'idx' not in test_df:
        if 'file' in test_df:
            test_df['idx'] = [int(Path(_s).stem.split('_')[0]) for _s in test_df['file']]
        elif "idx_label" in test_df:
            logger.info(f'Renaming "idx_label" to "idx" for test_df...')
            test_df.rename(columns={"idx_label": "idx"}, inplace=True)          
        elif "idx_guess" in test_df:
            logger.info(f'Renaming "idx_guess" to "idx" for test_df...')
            test_df.rename(columns={"idx_guess": "idx"}, inplace=True)
        else:
            logger.error("test_df must have idx or idx_guess column")
            return
    test_df.set_index('idx', drop=False, append=False, inplace=True, verify_integrity=True)
    test_df.index = test_df.index.astype(int)
    test_df = test_df[['idx', metric]]
    test_df.rename(columns={metric:f'{metric}_test'}, inplace=True)
    
    train_df = get_midat(train_pkl, return_save_prefix=False)
    test_df[f'{metric}_train'] = train_df[metric].mean()

    if 'idx' not in train_df:
        if 'file' in train_df:
            train_df['idx'] = [int(Path(_s).stem.split('_')[0]) for _s in train_df['file']]        
        elif "idx_label" in train_df:
            logger.info(f'Renaming "idx_label" to "idx" for train_df...')
            train_df.rename(columns={"idx_label": "idx"}, inplace=True)            
        elif "idx_guess" in train_df:
            logger.info(f'Renaming "idx_guess" to "idx" for train_df...')
            train_df.rename(columns={"idx_guess": "idx"}, inplace=True)
        else:
            logger.error("train_df must have idx or idx_guess column")
            return
    train_df['idx'] = train_df['idx'].astype(int)

    pcc_df = pd.DataFrame({'threshold': thresholds})
    import gwplot
    for by in align_cols: # range(9, -1, -1)]:
        test_df['hasAlnIdentity'] = False
        train_mean_metrics = [] # this is a list of np.arrays

        if align_pkl is not None:

            align_df = get_midat(align_pkl, return_save_prefix=False)
            # key from align is from queryID (at least for foldalign which takes the first 13 characters from ID)
            align_df['idx'] = [int(queryid.split('_')[0]) for queryid in align_df['queryID']]

            identity_cols = [_s for _s in align_df.columns if _s.startswith('alnIdentity')]
            for col in identity_cols:
                test_df[f'{col}_mean'] = 0.0
                test_df[f'{col}_max']  = 0.0

            for idx, align_grp in align_df.groupby('idx'):
                idx = int(idx)
                logger.info(f'Processing idx:{idx} with id: {align_grp.iloc[0]["queryID"]}...')
                test_df.loc[idx,'hasAlnIdentity'] = True
                if test_df.loc[idx].name != idx:
                    logger.error(f'align_df idx: {idx} != eval_df.index: {test_df.loc[idx].index} !!!')

                for col in identity_cols:
                    test_df.loc[idx, f'{col}_mean'] = align_grp[col].mean()
                    test_df.loc[idx, f'{col}_max'] = align_grp[col].max()

        elif align_dir is not None:

            train_mean_metrics = misc.mpi_map(
                functools.partial(_get_thresholded_train_metrics_by_idx, by=by, thresholds=thresholds,
                                    align_dir=align_dir, train_df=train_df),
                tqdm(test_df.index))
            test_df['hasAlnIdentity'] = [_metric is not None for _metric in train_mean_metrics]
            # filter didn't work well below, not sure why, so use a list comprehension here
            train_mean_metrics = [_metric for _metric in train_mean_metrics if _metric is not None]

        else:
            logger.critical(f'No align_pkl or align_dir is provided!!!')
            return

        # reset index and keep only ones with AlignID
        # test_df.reset_index(inplace=True, drop=True)
        logger.info(f'Original number of samples in test_df: {len(test_df)}')
        test_df_aln = test_df[test_df['hasAlnIdentity'] == True].copy(deep=True)

        train_mean_metrics = np.stack(train_mean_metrics, axis=0) # []
        logger.info(f'Aligned number of samples in test_df: {len(test_df_aln)}, predict_f1s: {train_mean_metrics.shape}')

        # save the f1-seen values at selected thresholds
        for threshold in np.arange(0.0, 1.0, 0.05):
            idx = np.where(np.abs(thresholds - threshold) < 0.001)[0]
            if len(idx) == 0:
                logger.warning(f'Cannot find threshold: {threshold}')
                continue
            else:
                idx = idx[0]
            # test_df[f'f1_{by}_{int(threshold*100):02d}'] = train_mean_f1s[:, idx]
            test_df.loc[test_df['hasAlnIdentity'] == True, f'f1_{by}_{int(threshold*100):02d}'] = train_mean_metrics[:, idx]

        # calculate pcc values and save into a separate dataframe
        pccs = np.zeros_like(thresholds)
        pvals = np.zeros_like(thresholds)
        f1scales = np.zeros_like(thresholds)

        # convert all value exceptions to zero!
        test_df_aln.fillna(value=0.0, inplace=True) # there shouldn't be any!
        np.nan_to_num(train_mean_metrics, copy=False, nan=0.0, posinf=0.0, neginf=0.0)

        for i, threshold in enumerate(thresholds):
            itrue_preds = np.where(train_mean_metrics[:,i] > 0.0)[0]
            if len(itrue_preds) > 1:
                pccs[i], pvals[i] = pearsonr(test_df_aln.iloc[itrue_preds][f'{metric}_test'], train_mean_metrics[itrue_preds,i])
                # pccs[i], pvals[i] = pearsonr(test_df['f1'], train_mean_f1s[:,i])
                f1scales[i] = test_df_aln.iloc[itrue_preds][f'{metric}_test'].mean() / train_mean_metrics[itrue_preds,i].mean()
            else:
                pccs[i] = np.NaN
                pvals[i] = np.NaN
                f1scales[i] = np.NaN

        pcc_df[f'pcc_{by}'] = pccs
        pcc_df[f'pvalue_{by}'] = pvals
        pcc_df[f'f1scale_{by}'] = f1scales

        # store the f1_align with the best pcc
        # imax = df['pcc'].argmax()
        # test_df['f1_align'] = train_mean_f1s[:, imax]
        fig = gwplot.ply_dfs_xy(pcc_df, x='threshold', y=f'pcc_{by}', fmt='line', names=by, fig=fig, show=False)
        # gwp.ply_2d(test_df, x='f1', y='f1_align', fmt='scatter', show=True)

    if align_program is None:
        align_program = align_dir if align_pkl is None else align_pkl[0] if isinstance(align_pkl, list) else align_pkl
        align_program = Path(align_program).stem.split('_')[-1]

    save_prefix = misc.get_1st_value([save_prefix, f'{auto_save_prefix}_alnID_{align_program}', 'autoname'])
    pcc_df.to_csv(f'{save_prefix}_pcc.csv', index=False)

    if show:
        fig.show()
    gwplot.ply_save_image(fig, save_prefix=save_prefix)

    if kwargs.get('save_lumpsum', False):
        # for threshold in np.arange(0.0, 1.0, 0.05):
        #     idx = np.where(np.abs(thresholds - threshold) < 0.001)[0]
        #     if len(idx) == 0:
        #         logger.warning(f'Cannot find threshold: {threshold}')
        #         continue
        #     else:
        #         idx = idx[0]
        #     test_df[f'f1_align_{int(threshold*100):02d}'] = train_mean_f1s[:, idx]
        kwargs.setdefault('save_pkl', True)
        kwargs.setdefault('save_lumpsum', True)
        save_lumpsum_files(test_df, save_prefix=f'{save_prefix}_{metric}', **kwargs)

    return fig


def append_train_f1_scores(
        align_pkl,
        train_pkl=None,
        test_pkl=None,
        align_cols=['alnIdentity'] + [f'alnIdentity{i}' for i in range(10)],
        join_on='idx',
        save_prefix=None,
        **kwargs):
    """ append train f1 scores to test_vs_train alignment results of a single test sequence"""

    align_df, auto_save_prefix = get_midat(align_pkl, return_save_prefix=True)
    auto_save_prefix += '.f1'
    idx = int(align_df.iloc[0]['queryID'].split('_')[0])
    idx_from_file = int(Path(align_pkl).stem.split('_')[0])
    if idx != idx_from_file:
        logger.critical(f'idx: {idx} != idx_from_file: {idx_from_file} for align_pkl: {align_pkl} !!!')
    # get idx as in train_df from the "templID" column in align_df  
    align_df['idx'] = [int(templid.split('_')[0]) for templid in align_df['templID']]
    
    if test_pkl is not None:
        test_df = get_midat(test_pkl, return_save_prefix=False)
        # get the f1 value from test_df and append to align_df
        f1 = test_df[test_df['idx'] == idx]['f1'].values[0]
        align_df['f1_test'] = f1
        logger.info(f'Appending f1_test: {f1} to align_df ...')
        auto_save_prefix += f'_{f1:.3f}'

    train_df = get_midat(train_pkl, return_save_prefix=False)
    # set the 'idx' column of train_df as index
    # train_df.set_index('idx', drop=False, append=False, inplace=True, verify_integrity=True)
    # train_df.index = train_df.index.astype(int)

    # assign the f1_train value from train_df to align_df by matching 'idx'
    # align_df['f1_train'] = train_df.loc[align_df['idx'].values]['f1']
    align_df = pd.merge(align_df, train_df[['idx', 'f1']], on='idx', copy=True, how="inner", indicator=False, suffixes=[None, '_train'])
    
    if kwargs.get('save_lumpsum', False):
        kwargs.setdefault('save_pkl', True)
        save_prefix = misc.get_1st_value([save_prefix, auto_save_prefix])
        save_lumpsum_files(align_df, save_prefix=save_prefix, **kwargs)
    else:
        logger.warning(f'Not saving lumpsum files!!!')
        

def _locofold_predict_single(ppmats, names=None, method='dbscan', eps=None, max_eps=0.5, min_eps=0.1, nsigma=1.0):
    """ predict the ctmat of a single sequence by bootstrapping an ensemble of models """
    if names is not None:
        assert len(names) == len(ppmats), "names and ctmats must have the same length!"

    if len(ppmats) == 0:
        return dict(confidence=0.0, eps=eps, outlier_num=0, outlier_idx=[], outlier_f1=0.0, 
                    cluster_name='NULL', cluster_center=0.0, cluster_std=0.0, 
                    cluster_min=0.0, cluster_max=0.0, cluster_ppmat=None)
    
    # F1 score is used here (reverse of dist)
    pairwise_f1 = np.zeros((len(ppmats), len(ppmats)), dtype=float)
    for i in range(len(ppmats)):
        for j in range(i):
            pairwise_f1[i,j] = mitas_utils.pfarm_metric(ppmats[i], ppmats[j], keep_batchdim=False)[1]
            pairwise_f1[j,i] = pairwise_f1[i,j]

    one_vs_rest = pairwise_f1.sum(axis=0) / (len(ppmats) - 1)
    idx_min_sim = np.argmin(one_vs_rest)

    ####### outlier detection: output the index of the outlier #######
    if eps is None:
        # eps = 1.0 / len(ppmats)
        # eps = one_vs_rest.mean() - one_vs_rest.std()
        eps = max([min([(1.0 - pairwise_f1[idx_min_sim].max()) * 0.98, max_eps]), min_eps])

    if len(ppmats) > 1.0 and method.upper() == 'DBSCAN':
        pairwise_dist = 1.0 - pairwise_f1

        np.fill_diagonal(pairwise_dist, 0.0)
        cluster_fn = cluster.DBSCAN(eps=eps, min_samples=len(pairwise_dist) - 2, metric='precomputed')
        cluster_out = cluster_fn.fit(pairwise_dist)

        labels = np.array(cluster_out.labels_, dtype=int)
        outlier_idx = np.where(labels == -1)[0]
        n_clusters = len(np.unique(labels)) - (1 if len(outlier_idx) else 0) # exclude outliers

        if n_clusters == 0: # no cluster found, simply average everything
            logger.debug(f'No cluster found in the pairwise distance matrix!!!')
            outlier_idx = []
        elif n_clusters > 1: # use the largest cluster only, everything else as outlier
            # logger.warning(f'Found {n_clusters} clusters in the pairwise distance matrix!!!')
            cluster_sizes = np.bincount(labels + 1)
            biggest_cluster_idx = np.argmax(cluster_sizes[1:]) # ignore the outliers with label=-1
            outlier_idx = np.where(labels != biggest_cluster_idx)[0]
    elif len(ppmats) > 1:
        outlier_idx = np.where(pairwise_f1.sum(axis=0) / (len(ppmats)-1) < eps)[0]
        if len(outlier_idx) == len(ppmats):
            outlier_idx = []
        # sorted_idx = np.argsort(pairwise_vec)
        # outlier_idx = sorted_idx[0:1]
    else:
        outlier_idx = []

    # easy target: very high similarity for all pairs
    if len(outlier_idx) != 1 and one_vs_rest[idx_min_sim] > 0.88:
        outlier_idx = [idx_min_sim]

    # deal with the case of very low similarity for all pairs
    if one_vs_rest.max() < 0.6 and len(outlier_idx) == 1:
        outlier_idx = []

    ####### Compute the cluster mean, std, min, max, and the outlier f1 #######
        
    if len(outlier_idx) > 0:
        outlier_f1 = pairwise_f1[:, outlier_idx].sum(axis=0) / (len(ppmats) - 1)
        cluster_mat = np.delete(pairwise_f1, outlier_idx, axis=0)
        cluster_mat = np.delete(cluster_mat, outlier_idx, axis=1)
    else:
        outlier_f1 = np.empty((0,)) 
        cluster_mat = pairwise_f1
    # sometimes there are two outliers, so we need to check both
    # if (one_vs_rest < outlier_f1).sum() > 1:
    #     outlier_idx = np.where(one_vs_rest == outlier_f1)[0][0]
    #     outlier_f1 = one_vs_rest[outlier_idx]

    cluster_size = len(cluster_mat)
    cluster_mean = cluster_mat.sum () / (cluster_size*cluster_size - cluster_size + 1e-6)
    
    np.fill_diagonal(cluster_mat, cluster_mean) # so that std would be zero for diagonal elements
    
    cluster_min = cluster_mat.min()
    cluster_max = cluster_mat.max()
    cluster_std = np.sqrt(np.square(cluster_mat - cluster_mean).sum() / (cluster_size*cluster_size - cluster_size - 1 + 1e-6)) 

    # print(f'f1_min: {f1min}; cluster min: {clstr_mean}; cluster std: {clstr_std}')
    # outlier_nsigma = (cluster_mean - np.mean(outlier_f1)) / (cluster_std + 1e-6)

    # if method.upper() not in ['DBSCAN']:
    #     if abs(outlier_nsigma) >= nsigma and cluster_mean > min_dist:# and outlier_f1 < 0.8:
    #         # logger.info(f'The target sequence is estimated to be of type: {imin}')
    #         outlier_found = True
    #     else:
    #         outlier_found = False
    #         outlier_idx = []
            # logger.info('The target sequence is out of distribution wrt the training set!')    
        
    # average ctmats to get the final ctmat
    if len(outlier_idx) > 0:
        # remove the outlier from the list
        ppmats = [ppmats[i] for i in range(len(ppmats)) if i not in outlier_idx]
        
    ppmats = np.stack(ppmats, axis=0)
    ppmat_meta = ppmats.mean(axis=0)

    return dict(confidence=max([0.0, (cluster_mean - cluster_std) * np.exp(-2*cluster_std)]),
                eps=eps,
                outlier_num=len(outlier_idx),
                outlier_idx=outlier_idx,
                # outlier_nsigma=outlier_nsigma,
                outlier_f1=outlier_f1,
                cluster_id='UNK' if len(outlier_idx) == 0 else 
                          'Hybrid' if len(outlier_idx) > 1 else 
                           f'M{outlier_idx[0]}' if names is None else 
                           names[outlier_idx[0]],
                cluster_center=cluster_mean,
                cluster_std=cluster_std,
                cluster_min=cluster_min,
                cluster_max=cluster_max,
                cluster_ppmat=ppmat_meta)


def run_linearfold(seqs, exe_path='./LinearFold/linearfold', MFE=True, **kwargs):
    """ run linearfold for a list of sequences """

    # run linearfold
    if not os.path.isfile(exe_path):
        logger.error(f'LinearFold executable not found: {exe_path} !!!')
        return

    if isinstance(seqs, str):
        seqs = [seqs]

    cmd_list = [exe_path]
    if MFE:
        cmd_list.append('-V') # '--Vienna'

    proc = subprocess.run(cmd_list, input='\n'.join(seqs),
                          stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    if proc.returncode != 0:
        logger.error(f'LinearFold failed to run with error: {proc.stderr} !!!')
        return {}

    # parse the output
    linearfold_out = proc.stdout.split()
    assert len(linearfold_out) == 3 * len(seqs), "LinearFold output must have three string outputs per seq!!!"

    return {'seqs': linearfold_out[::3], 'dbns': linearfold_out[1::3], 'mfes': [float(_s[1:-1]) for _s in linearfold_out[2::3]]}


def ctmat_moefold_predict(
        loco_output = 'metafam3.metafam2d_nr80_test.eval/eval_all.pkl',
        # pred_file='bprnaNEW.bprnaNEW.eval/eval_all.pkl',
        # pred_file='bprnaTR0.TS0.eval/eval_all.pkl',
        # pred_file='strive.libset_len30-600_nr80_SRP.eval/eval_all.pkl',
        loco_home = Path.home() / "bench/contarna/seq2ct_lstmconv2d",
        loco_prefix='metafam3.metafam2d_SUB_nr80_test_nr80.l4c64.validobj.',
        # loco_prefix='metafam3.metafam2d_SUB_nr80_test_nr80_nr90.l4c64.validobj',
        # loco_prefix='strive.libset_len30-600_nr80.l4c64',
        loco_types=["all",  "tRNA", "RNaseP", "gpI-intron", "23S-rRNA", "5S-rRNA", "tmRNA", "16S-rRNA", "TERC", "SRP"],
        loco_suffix='.upsample',
        phys_dir=None, # pre-calculated results from MFE models, Path.home() / "database/contarna/mxfold2_2021/bpRNAnew_dataset",
        phys_models=['linearfold'], # 'rnafold', 'simfold', 'rnastructure'],
        phys_exes=None,
        eps=None,         # the mininum distance for clustering (None: auto-guess by the mean of the pairwise f1 scores)
        nsigma=0.3,       # the threshold for outlier detection
        save_dir=None,
        save_prefix=None,
        save_lumpsum=True,
        save_individual=False,
        **kwargs):
    """ MoEFold2D workflow for predicting ctmat and RNA family """
    loco_home = Path(loco_home)
    if save_dir is None:
        save_dir = loco_home / f'{loco_prefix}moefold{loco_suffix}'
    if save_prefix is None:
        save_prefix = f'moefold_eps-{"auto" if eps is None else eps}' # _nsigma-{nsigma}'
        # if Path(loco_output).parent.stem:
            # save_prefix = f'{Path(loco_output).parent.stem}.{save_prefix}'

    # read predicted ctmat by all individual models
    loco_dfs = []
    loco_names = []
    sample_ids = []
    for loco_type in loco_types:
        pkl_file = loco_home / f'{loco_prefix}{loco_type}{loco_suffix}' / loco_output
        if pkl_file.is_file():
            loco_df = get_midat(pkl_file, info=False, return_save_prefix=False)
            loco_dfs.append(loco_df)
            loco_names.append(loco_type)
            sample_ids.extend(loco_df['idx'].tolist())
        else:
            logger.warning(f'{pkl_file} is not a valid file!!!')

    # get unique sample ids
    sample_ids = list(set(sample_ids))
    sample_ids.sort()
    logger.info(f'Found {len(sample_ids)} unique samples in the test set!!!')

    # .ct stores the LxL ctmat (assumes each df contains the same samples in the same order)
    ctmats_list = [[_df.iloc[_i]['ct' if 'ct' in _df else 'ppmat_guess'] for _df in loco_dfs] for _i in range(loco_dfs[0].shape[0])]

    moefold_df = misc.mpi_map(
        functools.partial(_locofold_predict_single, names=loco_names, eps=eps, nsigma=nsigma),
        ctmats_list,
        )

    moefold_df = pd.DataFrame(moefold_df)
    moefold_df.rename(columns={"cluster_ppmat": "ppmat_loco"}, inplace=True)

    # copy some columns from the first df
    for col in ['moltype', 'id', 'file', 'len', 'seq', 'idx']:
        if col in loco_df:
            moefold_df.insert(0, col, loco_df[col])

    # get metrics if available
    # add the ground truth metrics from each model as a new column
    for loco_type, loco_df in zip(loco_names, loco_dfs):
        if 'f1' in loco_df:
            moefold_df[f'f1_{loco_type}'] = loco_df.f1

    # compute the metric (F1 score) for the averaged ctmat (maybe we see an increase here!)
    if 'ct_label' in loco_df:
        ct_labels = [molstru.ct2ctmat(ds.ct_label, l=ds.len_label) for _, ds in loco_df.iterrows()]
        moefold_df['ctmat_label'] = ct_labels
        moefold_df['f1_loco'] = [mitas_utils.pfarm_metric(moefold_df.iloc[_i].ppmat_loco, ct_labels[_i], keep_batchdim=False)[1] for _i in range(len(moefold_df))]
        moefold_df['f1_loco_ct'] = [mitas_utils.pfarm_metric((moefold_df.iloc[_i].ppmat_loco > 0.5).astype(int), ct_labels[_i], keep_batchdim=False)[1] for _i in range(len(moefold_df))]
    
    if phys_dir is not None:  # pre-calculated results from MFE models (usually with known labels for developing the consensus analysis)
        phys_dir = Path(phys_dir)
        test_stem = Path(Path(loco_output).parent.stem).suffix.lstrip('.')

        phys_cols = []
        for phys_mod in phys_models:
            phys_pkl = phys_dir / f'{test_stem}.{phys_mod}_vs_label.pkl'
            if phys_pkl.is_file():
                loco_df = get_midat(phys_pkl, info=False)
                ct_name = f"ct_{phys_mod}"
                loco_df.rename(columns={"ct_guess" if "ct_guess" in loco_df else "ct": ct_name,
                                "idx_guess": "idx",
                                "f1": f"f1_{phys_mod}"}, inplace=True)
                moefold_df = pd.merge(moefold_df, loco_df[["idx", ct_name, f"f1_{phys_mod}"]], on='idx', 
                                        copy=True, how="left", indicator=False, suffixes=[None, f'_{phys_mod}'])
                phys_cols.append(ct_name)
                # count the number of NaNs in the ct_name column
                logger.info(f'Found {sum(moefold_df[ct_name].isna() | moefold_df[ct_name].isnull())} NaN/None values in col: {ct_name} in the merged pkl!!!')
            else:
                logger.warning(f'{phys_pkl} is not a valid file!!!')

        # .ct stores the LxL ctmat (assumes each df contains the same samples in the same order)
        ctmats_list = [[molstru.ct2ctmat(ds[phys_mod], l=ds['len']) for phys_mod in phys_cols if isinstance(ds[phys_mod], np.ndarray)] for _, ds in moefold_df.iterrows()]

        moefold_df.drop(columns=phys_cols, inplace=True)

        phys_metafold = misc.mpi_map(
            functools.partial(_locofold_predict_single, names=None, eps=eps, nsigma=nsigma),
            ctmats_list,
            )
        phys_metafold = pd.DataFrame(phys_metafold)
        # rename the columns
        phys_metafold.rename(columns={"cluster_ppmat": "ppmat_phys"}, inplace=True)
        if 'ct_label' in loco_df:
            phys_metafold['f1_phys'] = [mitas_utils.pfarm_metric(phys_metafold.iloc[_i].ppmat_phys, ct_labels[_i], keep_batchdim=False)[1] 
                                        if phys_metafold.iloc[_i].ppmat_phys is not None else None for _i in range(len(phys_metafold))]
            phys_metafold['f1_phys_ct'] = [mitas_utils.pfarm_metric((phys_metafold.iloc[_i].ppmat_phys > 0.5).astype(int), ct_labels[_i], keep_batchdim=False)[1]
                                        if phys_metafold.iloc[_i].ppmat_phys is not None else None for _i in range(len(phys_metafold))]
                                    
        moefold_df = pd.merge(moefold_df, phys_metafold, left_index=True, right_index=True, how='left', indicator=False, suffixes=[None, '_phys'])
    elif phys_exes is not None: # run the MFE models (usually when the label is unknown in predition mode)
        if isinstance(phys_exes, str):
            phys_exes = [phys_exes]
        for phys_exe in phys_exes:
            if phys_exe.endswith('linearfold'):
                phys_res = run_linearfold(moefold_df['seq'].tolist(), exe_path=phys_exe)
                moefold_df['ppmat_phys'] = [molstru.ct2ctmat(molstru.dbn2ct(_dbn), l=len(_dbn)) for _dbn in phys_res['dbns']]
            else:
                logger.warning(f'Unsupported MFE model: {phys_exe}!!!')
    else:
        logger.warning(f'No MFE models are provided!!!')
    
    assert 'ppmat_loco' in moefold_df, 'ppmat_loco must be in the dataframe!!!'
    assert 'ppmat_phys' in moefold_df, 'ppmat_phys must be in the dataframe!!!'
    moefold_df['ppmat'] = moefold_df.apply(lambda row: row['ppmat_phys'] if row['cluster_id'] in ['UNK', 'Hybrid'] else row['ppmat_loco'], axis=1)
    moefold_df['ctmat'] = moefold_df.apply(lambda row: (row['ppmat'] > 0.5).astype(int), axis=1)
    moefold_df['ct'] = moefold_df.apply(lambda row: molstru.ctmat2ct(row['ctmat']), axis=1)
    moefold_df['dbn'] = moefold_df.apply(lambda row: molstru.ct2dbn(row['ct'], l=len(row['seq'])), axis=1)
    if 'f1_loco' in moefold_df and 'f1_phys' in moefold_df:
        moefold_df['f1'] = moefold_df.apply(lambda row: row['f1_phys'] if row['cluster_id'] in ['UNK', 'Hybrid'] else row['f1_loco'], axis=1)
        moefold_df['f1_ct'] = moefold_df.apply(lambda row: row['f1_phys_ct'] if row['cluster_id'] in ['UNK', 'Hybrid'] else row['f1_loco_ct'], axis=1)

    if save_lumpsum:
        save_lumpsum_files(moefold_df, save_dir=save_dir, save_prefix=f'{save_prefix}',
                        save_pkl=True, save_csv=True, csv_exclude=['ppmat_loco', 'ctmat_label', 'ppmat_phys', 'seq', 'ppmat', 'ctmat', 'ct', 'dbn'])
    if save_individual:
        save_individual_files(moefold_df, save_dir=save_dir, save_genre=['ct', 'dbn'], named_after='id')


def ctmat_consensus(ctmats, method=None):
    """ obtain concensus ctmat from multile predictions/models (not yet implemented)"""
    pass


def ctmat_guess_vs_label(
        *data_names,          # guess.pkl, [label.pkl] (guess.pkl must contain all cols without label.pkl)
        on='file',            # matching col (file.stem used; nonstandard residues could differ if using seq)
        guess_on=None,        # default order: [on]_guess, [on] (not implemented yet)
        label_on=None,        # default order: [on]_label, [on] (not implemented yet)
        check_on='seq',       # the key to check consistency
        val='ct',             # the column to be converted to ctmat (not implemented yet)
        guess_val=None,       # default order: [val]_guess, [val] (not implemented yet)
        label_val=None,       # default order: [val]_label, [val] (not implemented yet)
        min_len=None,         # not yet implemented
        max_len=None,         # not yet implemented
        threshold=0.5,        # whether to apply threshold for ppm when first calulating metrics
        post_process=None,    # for guess ctmat only: canonical (cn), noncanonica (nc), ufold_cn, ufold_nc
        ufold_threshold=None, # the threshold used by ufold_postpro
        min_delta_ij=None,    # apply mininum distance cutoff for paring residues i and j
        min_stem_len=None,    # apply mininum stem length cutoff
        grid_search=False,    # whether to do grid search of the threshold for ppm
        save_result=False,
        save_dir='./',
        save_prefix=None,
        **kwargs):
    """ post-process guess and benchmark against label """
    args = misc.Struct(locals())
    logger.debug('Arguments:\n' + gwio.json_str(args.__dict__))

    if isinstance(data_names, str) or len(data_names) == 1:
        guess_label, auto_save_prefix = get_midat(data_names, return_save_prefix=True)
        if auto_save_prefix is not None: auto_save_prefix += '_finetuned'
    else:
         guess_label, auto_save_prefix = merge_guess_label_dfs(*data_names, on=on, 
            guess_on=guess_on, label_on=label_on, check_on=check_on)

    # collect seq and guess data
    seqs_data = guess_label['seq_label' if 'seq_label' in guess_label else 'seq'].to_list()
    if 'ppmat_guess' in guess_label.columns:
        guess_data = guess_label['ppmat_guess'].to_list()
    elif 'ct_guess' in guess_label.columns:
        guess_data = misc.mpi_starmap(molstru.ct2ctmat, zip(
            guess_label['ct_guess'],
            guess_label['seq_guess'].str.len() if on != 'seq' else guess_label['seq'].str.len()),
            desc='guess ct2mat')
    else:
        logger.error(f'guess data must have either ppm or ct!!!')
        return

    # calculate metrics
    label_data = misc.mpi_starmap(molstru.ct2ctmat, zip(
        guess_label['ct_label' if 'ct_label' in guess_label else 'ct'],
        guess_label['seq_label' if 'seq_label' in guess_label else 'seq'].str.len() ),
        desc='label ct2mat')

    logger.info(f'Calculating metrics of the input data ...')
    pfarm_fn = functools.partial(mitas_utils.pfarm_metric, keep_batchdim=False, beta=1.0, threshold=threshold)
    pfarm_mat = misc.mpi_starmap(pfarm_fn, zip(guess_data, label_data), desc=f'run pfarm_metric with threshold={threshold}')
    pfarm_mat = np.stack(pfarm_mat, axis=0)
    print(pfarm_mat.mean(axis=0))

    # post-process the guess
    if any([post_process, min_delta_ij, min_stem_len]):
        guess_data = ctmat_post_process(guess_data, seqs_data, method=post_process, ufold_threshold=ufold_threshold,
            min_delta_ij=min_delta_ij, min_stem_len=min_stem_len, threshold=threshold)

        logger.info(f'Calculating metrics after post_process ...')
        pfarm_fn = functools.partial(mitas_utils.pfarm_metric, keep_batchdim=False, beta=1.0, threshold=threshold)
        pfarm_mat = misc.mpi_starmap(pfarm_fn, zip(guess_data, label_data), desc=f'run pfarm_metric with threshold={threshold}')
        pfarm_mat = np.stack(pfarm_mat, axis=0)
        print(pfarm_mat.mean(axis=0))

    # perform grid search and re-calculate metrics
    if grid_search:
        threshold_by_grid_search = pfarm_grid_search_threshold(guess_data, label_data)
        # calcualte metrics again
        logger.info(f'Calculating metrics after grid search ...')
        pfarm_fn = functools.partial(mitas_utils.pfarm_metric, batch=False, beta=1.0, threshold=threshold_by_grid_search)
        pfarm_mat = misc.mpi_starmap(pfarm_fn, zip(guess_data, label_data), desc=f'run pfarm_metric with threshold={threshold_by_grid_search}')
        pfarm_mat = np.stack(pfarm_mat, axis=0)
        print(pfarm_mat.mean(axis=0))

    # save the metrics to dataframe
    guess_label = guess_label.assign(
        pre=pfarm_mat[:,0],
        f1 =pfarm_mat[:,1],
        acc=pfarm_mat[:,2],
        rec=pfarm_mat[:,3],
        mcc=pfarm_mat[:,4],
        )

    if save_result:
        save_prefix = misc.get_1st_value([save_prefix, auto_save_prefix, 'guess_vs_label'])
        save_path = Path(save_prefix) if save_dir is None else Path(save_dir) / save_prefix
        pkl_file = save_path.as_posix() + '.pkl'
        logger.info(f'Saving guess_label_df to: {pkl_file} ...')
        guess_label.to_pickle(pkl_file)
        # guess_label.to_csv(save_path.as_posix() + '.csv', index=False)
        # consider to call mitas_utils.save_all_files() to save individual files

    return guess_label


def pfarm_grid_search_threshold(guess, label, grid_min=0.0, grid_max=1.0, grid_step=0.02, grid_num=None,
        tolerance=0.001, max_iter=100, metric='f1', direction='maximize', **kwargs):
    """ grid-search the threshold to get pfarm metrics """

    pfarm_labels = ['pre', 'f1', 'acc', 'rec', 'mcc']
    imetric = pfarm_labels.index(metric)

    i, metric_old, metric_new = 0, -1., 0
    while metric_new - metric_old > tolerance and i < max_iter:
        i += 1
        logger.info(f'Iter#{i}: grid_min: {grid_min:0.4f}, grid_max: {grid_max:0.4f}, grid_step: {grid_step:0.4f}')
        pfarm_means = []
        if grid_num is None:
            thresholds = np.arange(grid_min, grid_max + grid_step, grid_step)
        else:
            thresholds = np.linspace(grid_min, grid_max, grid_num)

        for threshold in thresholds:
            pfarm_fn = functools.partial(mitas_utils.pfarm_metric, threshold=threshold, batch=False, **kwargs)
            pfarm_mat = np.stack(list(itertools.starmap(pfarm_fn, zip(guess, label))), axis=0)
            # pfarm_result = misc.mpi_starmap(pfarm_fn, zip(guess_ct, label_ct), desc='run pfarm_metric')
            pfarm_mean = pfarm_mat.mean(axis=0)
            print(f'Threshold: {threshold:0.4f} - ', pfarm_mean)
            pfarm_means.append(pfarm_mean)

        pfarm_means = np.stack(pfarm_means, axis=0)

        if direction.lower() in ['maximize', 'max', 'up']:
            ibest = np.argmax(pfarm_means[:, imetric])
        else:
            ibest = np.argmin(pfarm_means[:, imetric])

        metric_old = metric_new
        metric_new = pfarm_means[ibest, imetric]
        threshold = thresholds[ibest]
        logger.info(f'Best threshold: {threshold:0.4f}, metric_old: {metric_old:0.4f}, metric_new: {metric_new:0.4f}')
        if threshold - grid_min > grid_max - threshold:
            grid_min = (grid_min + threshold) / 2
        else:
            grid_max = (grid_max + threshold) / 2
        grid_step /= 2

    logger.info(f'Found best metric: {metric}={metric_new:0.4f} with threshold: {threshold:0.4f}')
    return threshold


def ctmat_post_process(guess, seq, method=None, ufold_threshold=0.99, min_delta_ij=None, min_stem_len=None, threshold=0.5, seqs_len=None):
    """ guess and input have batch_size as the first dim
        threshold is for min_stem_len cutoff which requires binary labels for the bpmat
    """
    if method is None:
        pass
    elif method in ['cn', 'canonical']:
        bpmat = misc.mpi_map(functools.partial(molstru.seq2bpmat_just_pairs, canonical=True,
                min_delta_ij=min_delta_ij, min_stem_len=min_stem_len, return_energy=False),
                tqdm(seq, desc='seq2bpmat_just_pairs'), quiet=True)
        guess = [guess[i] * bpmat[i] for i in range(len(guess))]
    elif method in ['nc', 'nocanonical', 'noncanonical']:
        bpmat = misc.mpi_map(functools.partial(molstru.seq2bpmat_just_pairs, canonical=False,
                min_delta_ij=min_delta_ij, min_stem_len=min_stem_len, return_energy=False),
                tqdm(seq, desc='seq2bpmat_just_pairs'), quiet=True)
        guess = [guess[i] * bpmat[i] for i in range(len(guess))]
    elif method.startswith('ufold'):
        import ufold_postpro as ufold

        res2onehot_dict = {  # A? U? C? G?
            '-': np.array([0.0, 0.0, 0.0, 0.0], dtype=np.float32), # required for residue_nn feature
            'A': np.array([1.0, 0.0, 0.0, 0.0], dtype=np.float32),
            'U': np.array([0.0, 1.0, 0.0, 0.0], dtype=np.float32),
            'T': np.array([0.0, 1.0, 0.0, 0.0], dtype=np.float32),
            'C': np.array([0.0, 0.0, 1.0, 0.0], dtype=np.float32),
            'G': np.array([0.0, 0.0, 0.0, 1.0], dtype=np.float32)
            }

        seqs_len = [len(_seq) for _seq in seq]
        max_seqlen = max(seqs_len)
        seq_onehot = misc.mpi_map(functools.partial(molstru.seq2onehot, length=max_seqlen), seq)
        seq_onehot = np.stack(seq_onehot, axis=0)

        print('seq_onehot shape', seq_onehot.shape)

        guess = misc.mpi_map(functools.partial(mitas_utils.fix_length2d, length=[max_seqlen]), guess)
        guess = np.stack(guess, axis=0)

        guess_logits = -np.log((1.0 + 1e-6) / (guess + 1e-6) - 1.0) # pre-sigmoid logits
        constraint_fn = ufold.constraint_matrix_batch

        if method in ['ufold_nc', 'ufold_all']:
            constraint_fn = ufold.constraint_matrix_batch_addnc
        elif method in ['ufold_canon', 'ufold_canonical', 'ufold_cn']:
            constraint_fn = ufold.constraint_matrix_batch
        else:
            logger.warning(f'Unrecognized return_postpro method: {method}, use ufold_canonical by default!')

        guess = ufold.post_process_paddle(
            guess_logits, seq_onehot, threshold=ufold_threshold, constraint_fn=constraint_fn,
            lr_min=0.01, lr_max=0.1, num_itr=100, rho=1.6, with_l1=True)
            #seq_ori, 0.01, 0.1, 50, 1, True))
        # guess = np.to_tensor(guess)
        # the following is used by UFold
        # the threshold is
        # u_no_train = postprocess(pred_contacts,
        #     seq_ori, 0.01, 0.1, 100, 1.6, True, 1.5) sigmoid(1.5) = 0.82
        #     #seq_ori, 0.01, 0.1, 50, 1, True)
        # map_no_train = (u_no_train > 0.5).float()

        guess = [guess[i,:seqs_len[i],:seqs_len[i]] for i in range(len(guess))]

    else:
        logger.warning(f'Unrecognized return_postpro method: {method}!')

    if min_delta_ij:
        guess = misc.mpi_map(functools.partial(molstru.bpmat_clip_delta_ij, min_delta_ij=min_delta_ij), guess)

    if min_stem_len:
        guess_bpmat = [(_ctmat > threshold).astype(int) for _ctmat in guess]
        guess_bpmat = misc.mpi_map(functools.partial(molstru.bpmat_clip_stem_len, min_stem_len=min_stem_len), guess_bpmat)
        guess = [guess[i] * guess_bpmat[i] for i in range(len(guess))]

    return guess


if __name__ == '__main__':

    misc.argv_fn_caller(sys.argv[1:], verbose=1) # module=sys.modules[__name__]